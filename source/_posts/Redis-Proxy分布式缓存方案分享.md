---
title: Redis-Proxy分布式缓存方案分享
urlname: redis-proxy-distributed-cache-project
date: 2020-04-20 18:56:46
category: 工具
tags: redis
photos: /images/RedisProxy-structure.png
---

## 前言

想必在大家的架构中，Redis 已经是一个必不可少的部件，丰富的数据结构、超高的性能以及简单的协议，让它能够很好的作为数据库的上游缓存层。正是人们大量使用 Redis，逐渐暴露了一些缺点：
1. 单节点物理内存是有限的，不能满足日益增长的业务内存的需要；
2. 不能提供高可用服务。节点挂掉或者该节点机器宕机，导致大量请求走向数据库，造成数据库压力增大，加大数据库宕机风险；
3. 运维会为每个业务方创建一个节点。各业务方内存分配不均匀；机器部署杂、多，请求入口多，运维管理复杂、难以维护等。

<!-- more -->

于是，人们将目光投向了分布式存储。市面上的分布式存储主要包括两大类：
一类是在服务端做的分片，由运维手动给服务端每个实例分槽，服务端再根据请求的 key 所对应的槽找到对应的机器，代表服务是 redis-cluster；
另一类是在客户端和服务端之间做一层代理服务，由代理服务根据一致性 hash 算法将请求路由到指定机器，代表服务是 twemproxy 和 codis。

## 问题提出

集群服务相比较单点有了很好的扩展性，但是在使用过程中，弊端逐渐凸显（拿 codis 举例）：

1. 业务方混用。所有的业务方都混用一个集群，只要有一个业务方不正当使用集群，造成集群处理速度明显变慢，这样其他所有业务方都会受到影响；
2. key 冲突。在混用同一个集群时，会有 key 冲突隐患。比如，业务方A和业务方B都使用用户的 userid 存储该用户的 session 信息，会造成整个业务错乱和产生大量脏数据；
3. 命令乱用。比如组里来个实习生，用了 `ZRANGE key 0 -1 [WITHSCORES]` 来请求 codis，如果这个有序集很大，会对网络和其他业务方造成影响；
4. bigkey 问题；
5. 问题定位复杂。要知道 codis 没有打印请求的日志，抓包耗时耗力；
6. 集群扩容难；
7. 集群快速恢复难。如果直接关闭业务方服务肯定不现实，这也导致我们迫切需要一个集群服务能够对业务方快速降级；
8. 难以统计。由于各个业务方请求的 key 都是散落在集群的每个节点上，业务方很难通过 redis 节点统计自己的请求量以及缓存命中率；
9. 不设置缓存时间。业务方在使用 codis 时如果不设置缓存的生命周期，导致随着时间的增长，无用的 key 越来越多，占用着大量的宝贵内存；
10. 连接数过大。线上大家在用同一个集群时，如果有一个业务方不靠谱，占用了集群所有的连接，导致系统异常；
11. 不能生成报表。业务方在使用 codis 时，不知道自己请求 key 的次数、不知道自己缓存命中率，可以说对自己使用的 codis 很难知道自己请求的整体情况。

以上比如混用、key 冲突、value 大等问题不是说一定会导致出现问题，但是这是安全隐患，而且一直存在，根据*墨菲定律*，这种隐患肯定会在以后发生。并且很难通过运维规范来阻止这类事情的发生，这就迫切需要一个能解决上述问题的服务来替代市面上的集群。

## 解决思路

鉴于上章节中提到的种种问题，我们开发一个 redis-proxy 服务，通过在运营平台给每个业务方分配一个 source，打印日志，以及支持子集群等方法来解决上述问题。

1. 混用和扩容问题，可以用 redis-proxy 里面支持的子集群解决
a. 针对混用问题，我们线上搭建两个子集群 group1 和 group2，业务方A和B分别属于 group1 和 group2，这样业务方A和业务方B是物理隔离，如果A出了问题，不会影响业务方B；
b. 针对扩容问题，我们解决方式是搭建一个新的子集群。比如子集群 group1 快要用完了，我们可以搭建 group2，而且搭建和使用都不会影响 group1 里面的业务。

2. key 冲突、命令乱用、请求 value 大、集群难以恢复问题，通过分配 source 解决。
a. 针对 key 冲突问题，redis-proxy 服务会给每个业务方分配一个 source 作为业务方每个请求 key 的前缀，诸如 `{source}&key` 形式。这样每个业务方请求redis的key都不一样；
b. 针对命令乱用问题，我们会给每个业务方分配请求 redis 的命令，未分配的命令不给使用，这样可以有效管控命令；
c. 针对 bigkey 问题，我们会给每个业务方的每个请求限制请求最大值，大于这个值的，直接返回异常，不给存入集群；
d. 针对集群难以恢复问题，如果业务方严重影响子集群，对其他业务产生严重影响时，我们可以在运营平台给业务方快速降级；
e. 针对连接数占用严重问题，我们可以在运营平台限制每个业务方的连接数，防止因为异常导致 proxy 不稳定。

3. 针对定位问题复杂、业务方不设置缓存失效时间、报警和报表问题，通过打印日志来解决。
a. 针对定位问题复杂，我们打印日志，日志包括请求 key、source、耗时、命令、请求大小、返回大小、所属子集群等信息全面反映当时请求情况；
b. 针对业务方不设置缓存失效时间问题，我们可以让业务方请求的 key 强制失效，解决业务方因使用不当而占用线上大量内存的问题；
c. 针对报表问题，我们也可以通过日志统计报表，也可以在 Kibana 看到各个 source 各种命令的请求数、耗时以及命中率等信息。

根据上面的介绍，我们给出 redis-proxy 的标签：redis 协议、netty 框架、分布式、高性能、平台管理。

## 现状介绍

RedisProxy 作为一套分布式系统，由一个配置节点、多个访问代理节点、以及一系列主备服务器组成，配置节点我们称其为 config-server，访问代理节点为 proxy-server，一套 redis 主备服务器为一个 redis-group。

![日志统计](/images/RedisProxy-stat.jpg)