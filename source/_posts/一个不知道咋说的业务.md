---
title: 一个不知道咋说的业务
date: 2020-11-14 16:41:33
urlname: data-sync-design
category: 杂谈
---

[北森iTalent](https://www.italent.cn/)是我们公司采购的一个人力资源管理平台，它也是我们人事数据的来源。

现在需要通过时间窗接口获取变更的数据后，再生成文件到盖雅服务器里。

<!-- more -->

> 开始想把修改数据放内存中，然后直接写到对方文件里，但这样不够稳妥

决定在本地记录一份和目标一样的副本。先把数据在本地写好，再通过已有的服务来传过去，例如[《Golang接收上传文件》](/posts/golang-receive-upload-file.html)。测试一会之后，发现不用这么麻烦，改用 PHP 项目自己解决传递，见[《PHP传递文件》](/posts/php-send-file.html)。一个是减少了影响项目，另一个是利于维护。

> 但背景是目前的员工数量已经达到了 7k

先把增量员工查出来，再把数据切割成 csv 格式的字符串，写入文件。会出现如下问题：

- 1. 频繁使用远程连接会出现失败
- 2. 全量时接口会超时
- 3. 偶发的内容拼接错误（某一行出现在上一行的中间部分，其它行正常）

第一步考虑用队列先解决数据量大的问题，先优化为把需要同步的员工拆分为 100 人一批放入队列，把数据转换为一行文件内容后，先写入本地文件，再把整个本地文件内容复制到远程。

但是用的队列是多个同时消费，最后的结果没有按需要的顺序，并且需要区分每一个内容是属于哪次文件，在单次文件全部结束后再传递到服务器。

我决定以可用性更高的数据库方式来解决：新建一张表，用来存储员工在文件中对应的一行内容，再新增一个 Listener，在员工进行 CUD 时就新增或修改到新表中，定时任务再通过修改时间戳去查询。

此时把内容数组存在表中的 JSON 字段时，再取出来放到文件中时，顺序会错乱，是因为 MySQL 在存储 JSON 时按照 KEY 的字段长度做了排序，以便获得更好的存储性能：see [JSON比较](https://dev.mysql.com/doc/refman/5.7/en/json.html#json-comparison)

> To make lookups more efficient, it also sorts the keys of a JSON object. You should be aware that the result of this ordering is subject to change and not guaranteed to be consistent across releases.

所以就把内容字段类型改为 TEXT，切割好了再存进去，取的时候拼个换行符就能用了。这样子就不会出现多次进行远程连接，也留下了可以排查问题的记录。

> 业务可用性第一位
